{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges = {\n",
    "    'binance':\n",
    "        {\n",
    "            'bitcoin':'BTC',\n",
    "            'ethereum':'ETH',\n",
    "            'litecoin':'LTC',\n",
    "            'solana':'SOL',\n",
    "            'ripple':'XRP',\n",
    "            'dollar':'USDT',\n",
    "        },\n",
    "    'coinbase':\n",
    "        {\n",
    "            'bitcoin':'BTC',\n",
    "            'ethereum':'ETH',\n",
    "            'litecoin':'LTC',\n",
    "            'solana':'SOL',\n",
    "            #'ripple':'xrp',\n",
    "            'dollar':'USD',\n",
    "        }, \n",
    "    'ftx':\n",
    "        {\n",
    "            'bitcoin':'BTC',\n",
    "            'ethereum':'ETH',\n",
    "            'litecoin':'LTC',\n",
    "            'solana':'SOL',\n",
    "            'ripple':'XRP',\n",
    "            'dollar':'USD',\n",
    "        }, \n",
    "    'bitmex':\n",
    "        {\n",
    "            'bitcoin':'XBT',\n",
    "            'ethereum':'ETH',\n",
    "            'litecoin':'LTC',\n",
    "            'solana':'SOL',\n",
    "            'ripple':'XRP',\n",
    "            'dollar':'USD',\n",
    "        },\n",
    "    'kraken':\n",
    "        {\n",
    "            'bitcoin':'XBT',\n",
    "            'ethereum':'ETH',\n",
    "            'litecoin':'LTC',\n",
    "            'solana':'SOL',\n",
    "            'ripple':'XRP',\n",
    "            'dollar':'USD',\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 6\n",
    "date = datetime.datetime(2022, 2, 28)\n",
    "dates = [(date + datetime.timedelta(days=x)).strftime('%Y-%m-%d') for x in range(days)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### We use Binance for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "exch = 'binance'\n",
    "asset = 'ethereum'\n",
    "\n",
    "# time frequency\n",
    "fq = '10min'\n",
    "\n",
    "# concatenate all date over the 6 day period into single dataframe resampled by frequency\n",
    "ob_fq = pd.DataFrame()\n",
    "for date_str in dates:\n",
    "    try:\n",
    "        path = 'tardis_raw.nosync/'+exch+'/'+exch+'_book_snapshot_5_'+date_str+'_'+exchanges[exch][asset]+exchanges[exch]['dollar']+'.csv.gz'\n",
    "        ob = pd.read_csv(path, compression='gzip')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        path = 'tardis_raw.nosync/'+exch+'/'+exch+'_book_snapshot_5_'+date_str+'_'+exchanges[exch][asset]+'USDT'+'.csv.gz'\n",
    "        ob = pd.read_csv(path, compression='gzip')\n",
    "        pass\n",
    "    ob.sort_values(by='timestamp')\n",
    "    ob['datetime'] = pd.to_datetime(ob['timestamp'], unit='us')\n",
    "    ob_fq=ob_fq.append(ob.resample(fq, on='datetime', label='right').first())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute returns\n",
    "ob_fq['mid_price'] = (ob_fq['asks[0].price'] + ob_fq['bids[0].price'])/2\n",
    "ob_fq['returns'] = ob_fq['mid_price'].pct_change()\n",
    "ob_fq['binary_returns'] = ob_fq['returns'].apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets_stemmed_hashtags_no_stopwords.csv')\n",
    "tweets['datetime'] = pd.to_datetime(tweets.timestamp, infer_datetime_format=True)\n",
    "tweets = tweets.sort_values(by='datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample tweets with time frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (tweets['datetime'] > ob_fq['datetime'].iloc[5]) & (tweets['datetime'] <= ob_fq['datetime'].iloc[100])\n",
    "# tweets_fq = tweets[mask]\n",
    "\n",
    "tweets_fq = []\n",
    "for i in range(ob_fq.shape[0]-1):\n",
    "    tweets_one_period = []\n",
    "    #get all tweets between 2 timestamps\n",
    "    mask = (tweets['datetime'] > ob_fq['datetime'].iloc[i]) & (tweets['datetime'] <= ob_fq['datetime'].iloc[i+1])\n",
    "    tweets_filtered = tweets[mask]\n",
    "    if len(tweets_filtered)>0:\n",
    "        # EST CE QUE LES TWEETS DE LA PERIODE PERMETTENT DE PREDIRE LA PERF DE LA PROCHAINE PERIODE\n",
    "        try:\n",
    "            dt = ob_fq['datetime'].iloc[i+2]\n",
    "        except:\n",
    "            dt = ob_fq['datetime'].iloc[i+1]\n",
    "        tw = \" \".join([tweet for tweet in tweets_filtered['tweet_stemmer_hashtags_no_stopwords']])\n",
    "        tweets_fq.append([dt, tw])\n",
    "\n",
    "tweets_fq = pd.DataFrame(tweets_fq, columns=['datetime', 'tweets'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_fq.reset_index(drop = True, inplace = True)\n",
    "ob_fq.reset_index(drop = True, inplace = True)\n",
    "full_data = pd.merge(tweets_fq, ob_fq, on='datetime')\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(full_data.drop(['binary_returns'], axis=1), full_data[\"binary_returns\"], random_state=203, test_size=0.30)\n",
    "n_train = y_train.shape[0]\n",
    "n_test = y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vect = vectorizer.fit(df_train[\"tweets\"])\n",
    "x_train = vect.transform(df_train[\"tweets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the model on train dataset\n",
    "clf = LogisticRegression(penalty=\"none\")\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on train dataset + validation set (K-fold)\n",
    "pred_train = model.predict(x_train)\n",
    "pred_val = cross_val_predict(clf, x_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train dataset results :\n",
      "col_0            0   1\n",
      "binary_returns        \n",
      "0               42   0\n",
      "1                0  25\n",
      "Accuracy = 1.0\n",
      "\n",
      "--- validation set results :\n",
      "col_0            0   1\n",
      "binary_returns        \n",
      "0               31  11\n",
      "1               18   7\n",
      "Accuracy = 0.5671641791044776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, auc\n",
    "\n",
    "print(\"--- train dataset results :\")\n",
    "train_acc = accuracy_score(y_train, pred_train)\n",
    "cross_train = pd.crosstab(y_train, pred_train)\n",
    "print(cross_train)\n",
    "print(\"Accuracy = %s\" % train_acc)\n",
    "\n",
    "print(\"\\n--- validation set results :\")\n",
    "val_acc = accuracy_score(y_train, pred_val)\n",
    "cross_val = pd.crosstab(y_train, pred_val)\n",
    "print(cross_val)\n",
    "print(\"Accuracy = %s\" % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top and Flop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rbxsamurai', 'eth', 'blockchain', 'cryptocurrency', 'btc', 'tax', 'space', 'web3', 'cryptocurr', 'rbxs', 'xrp', 'rbx', 'gem', 'kibakrew', 'altseason', 'everrisev3', 'everriseedu', 'vektor', 'join', 'nft', 'bullish', 'digitalart', 'one', 'btt', 'matic', 'twitter', 'defi', 'take', 'metavers', 'gemanaliz', 'shibnobi_dojoswap', 'ama', 'market', 'kiba', 'nftdrop', 'cryptonews', 'blocvault', 'see', 'metaverse', 'saintpatricksday']\n",
      "['mri', 'f1', 'cro', 'whale', 'stock', 'platform', 'guy', 'messag', 'pitbull', 'way', 'think', 'check', 'staking', 'exchang', 'safemoon', 'forex', 'kishu', 'ripple', 'ani', 'prepar', 'shib', 'year', 'kasta', 'thi', 'binanc', 'biggest', 'altcoin', 'get', 'good', 'best', 'nftartist', 'pump', 'coin', 'shibnobi', 'saitama', 'bitcoin', 'trade', 'shinja', 'crypto', 'floki']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def get_coeffs(model, vect):\n",
    "    \"\"\"\n",
    "    Returns words / n_grams weights in ascending order.\n",
    "    (param) model : trained scikit learn model.\n",
    "    (param) vect : used count vectorizer.\n",
    "    return: word-weight pairwise\n",
    "    \"\"\"\n",
    "    words = vect.get_feature_names()\n",
    "    coeffs = model.coef_.tolist()[0]\n",
    "    coeff_df = pd.DataFrame({'word' : words, \n",
    "                        'coefficient' : coeffs})\n",
    "    coeff_df = coeff_df.sort_values(['word', 'coefficient'])\n",
    "    return coeff_df\n",
    "\n",
    "df_coeff = get_coeffs(model=model, vect=vect)\n",
    "df_sort = df_coeff.sort_values([\"coefficient\", \"word\"], ascending=False)\n",
    "top_40_words = df_sort.head(40)[\"word\"].tolist()\n",
    "flop_40_words = df_sort.tail(40)[\"word\"].tolist()\n",
    "print(top_40_words)\n",
    "print(flop_40_words)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
