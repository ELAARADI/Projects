{
 "cells": [
  {
   "attachments": {
    "master_203_pic.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![master_203_pic.jpg](attachment:master_203_pic.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Competition & Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Introduction project and features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of the project is to study the given data and use it in order to estimate the prices of the short term electricity futures(24h).\n",
    "However, putting aside the already high difficulty level of this task considering we're dealing with electricity prices, specifities of the dataset as well as the lack of  information will make it very difficult for us to have accurate predictions.\n",
    "\n",
    "We'll start by exploring the dataset and the nature of the features we're dealing with. \n",
    "\n",
    "Throughout the notebook, in order to compare predictions to the target, we will compare their distribution as well as their cumulative sum. The later isn't a natural go-to technique but we think that using the cumulative sum on a \"de_indexed\" series will give us a good, precise and concrete visulisation of the data series."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading modules  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the used functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ens(df_test, pred_test, save=True, path_save=\"y_test_prediction.csv\"):\n",
    "    \"\"\"\n",
    "    Export submissions with the good ENS data challenge format.\n",
    "    df_test : (pandas dataframe) test set\n",
    "    proba_test : (numpy ndarray) prediction as a numpy ndarray you get using method .predict()\n",
    "    save : (bool) if set to True, it will save csv submission in path_save path.\n",
    "    path_save : (str) path where to save submission.\n",
    "    return : dataframe for submission\n",
    "    \"\"\"\n",
    "    df_submit = pd.Series(pred_test, index=df_test.index, name=\"TARGET\")\n",
    "    df_submit.to_csv(path_save, index=True)\n",
    "    return df_submit\n",
    "\n",
    "def check_test(result, expected, display_error):\n",
    "    \"\"\"\n",
    "    Testing your results.\n",
    "    \"\"\"\n",
    "    if result == expected:\n",
    "        print(\"1 test passed.\")\n",
    "    else:\n",
    "        print(display_error)\n",
    "\n",
    "def features_to_correct(df):\n",
    "    res = []\n",
    "    filtered = []\n",
    "    for col in df.columns:\n",
    "        if not df[[col]][df[col].isna()].empty:\n",
    "            for i in df[[col]][df[col].isna()].index:\n",
    "                if i not in filtered:\n",
    "                    filtered.append(i)\n",
    "    return filtered\n",
    "\n",
    "# Signal creation for modeling the price\n",
    "def signal(df,col,time_period = 500,eps = 10,interval_around_eps=2):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with booleans & a string: \n",
    "    \n",
    "    True: value equivalent to a value that is much higher than the mean \n",
    "    recorded in the previous observations + eps + interval_around_eps\n",
    "\n",
    "    Stagnant: corresponds to a stagnant evolution (an evolution around eps +/- interval_around_eps)\n",
    "\n",
    "    False: signifies a slowdown of the evolution below the mean of \n",
    "    the observations in the previous time_period + eps - interval_around_eps\n",
    "    \"\"\"\n",
    "    \n",
    "    res = pd.DataFrame(np.zeros((len(df),1)),index=df.index,columns=['counter'])\n",
    "    for i in range(len(df)):\n",
    "        if abs(col.cumsum().iloc[i]) > abs(col.cumsum().iloc[max(i-time_period,0):i].mean()) + eps + interval_around_eps:\n",
    "            res.counter.iloc[i] = True\n",
    "        elif abs(col.cumsum().iloc[i]) > abs(col.cumsum().iloc[max(i-time_period,0):i].mean()) + eps - interval_around_eps and abs(col.cumsum().iloc[i]) < abs(col.cumsum().iloc[max(i-time_period,0):i].mean()) + eps + interval_around_eps:\n",
    "            res.counter.iloc[i] = 'stagnant'\n",
    "        else:\n",
    "            res.counter.iloc[i] = False\n",
    "            \n",
    "    return res.counter\n",
    "\n",
    "def signal_int(df,col,time_period = 500,eps = 10):\n",
    "    \"\"\"\n",
    "    simpler version of signal function, returns 1s and 0s depending on the current evolution: \n",
    "    \n",
    "    1: value equivalent to a value that is much higher than the mean \n",
    "    recorded in the previous observations + eps \n",
    "    \n",
    "    0: signifies a slowdown of the evolution below the mean of \n",
    "    the observations in the previous time_period + eps\n",
    "    \"\"\"\n",
    "    \n",
    "    res = pd.DataFrame(np.zeros((len(df),1)),index=df.index,columns=['counter'])\n",
    "    for i in range(len(df)):\n",
    "        res.counter.iloc[i] = int(abs(col.cumsum().iloc[i]) > abs(col.cumsum().iloc[max(i-time_period,0):i].mean()) + eps)\n",
    "    return res.counter\n",
    "\n",
    "def trend_divider(df,mask,signal=False):\n",
    "    \"\"\"\n",
    "    if signal = True -> returns a list with the order of trues and falses in the analysed dataframe:\n",
    "    if signal = False -> returns a list of dataframes seperating the trues from the falses \n",
    "        -> end up with a list of dataframe with each containing trues or falses keeping \n",
    "        the original indexes \n",
    "    \n",
    "    mask: is a the output of the signal function and is used to seperate the original dataframe into smaller datasets\n",
    "    \"\"\"\n",
    "    # \n",
    "    \n",
    "    # initialize variables\n",
    "    start_index = 0\n",
    "    current_mask = mask.iloc[0]\n",
    "    dataframes = []\n",
    "\n",
    "    # iterate through rows and create new dataframes when mask changes\n",
    "    for i in range(1, len(mask)):\n",
    "        if mask.iloc[i] != current_mask:\n",
    "            new_df = df.iloc[start_index:i]\n",
    "            if signal == True:\n",
    "                dataframes.append(new_df.iloc[0])\n",
    "            else:\n",
    "                dataframes.append(new_df)\n",
    "            start_index = i\n",
    "            current_mask = mask.iloc[i]\n",
    "\n",
    "    # create final dataframe\n",
    "    new_df = df.iloc[start_index:]\n",
    "    if signal:\n",
    "        dataframes.append(new_df.iloc[0])\n",
    "    else:\n",
    "        dataframes.append(new_df)\n",
    "    \n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def regression(trends,trend_signal,y_train_val,col,col_index,\n",
    "               prediction_x_data = pd.DataFrame([]),time_period = 500, eps = 10):\n",
    "    \"\"\"\n",
    "    Takes as input the split up dataframes as well as if its Trues or Falses, the Target, \n",
    "        the considered most important column (or set of columns), and if the user wants to\n",
    "        have a prediction, he needs to enter the test dataset inputs (df_test).\n",
    "    \n",
    "    Goes through the various trends (of True and False) in the trends input, and at each step,\n",
    "    if its an upward trend (trend_signal=True): the function will fit the specified features on the target using a non_stagnant model\n",
    "    if its not (trend_signal=False): the function calls another model, the stagnant model and fits the input on the target.\n",
    "    \n",
    "    We end up with two models:\n",
    "        -> a stagnant model which is a simple linear regression fitted using all the selected features on the target\n",
    "        -> a non_stagnant model which is also a linear regression, fitting the specific feature(s) on the target\n",
    "    \n",
    "    Later on, we will analyse the specific feature(s) in the test set, extract the trends, and following these trends,\n",
    "    we predict using the stagnant or non stagnant model.\n",
    "    \n",
    "    Returns a prediction on the y feature using the train set if no test set is input in the parameters\n",
    "    otherwise, it will return a prediction of y using the test set features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred = []\n",
    "    stagnant = LinearRegression()\n",
    "    non_stagnant = LinearRegression()\n",
    "\n",
    "    for i in range(len(trends)):\n",
    "        idx = trends[i].index\n",
    "        df = trends[i].reset_index(drop=True)\n",
    "        if not trend_signal[i]:\n",
    "            stagnant_model = stagnant.fit(df, y_train_val.filter(idx))\n",
    "            pred_train = stagnant_model.predict(df)\n",
    "            pred_train = pd.Series(pred_train, index=y_train_val.filter(idx).index)\n",
    "            pred.append(pred_train)\n",
    "        else:\n",
    "            non_stagnant_model = non_stagnant.fit(df[col], y_train_val.filter(idx))\n",
    "            pred_train = non_stagnant_model.predict(df[col])\n",
    "            pred_train = pd.Series(pred_train, index=y_train_val.filter(idx).index)\n",
    "            pred.append(pred_train)\n",
    "    pred_train = pd.concat(pred,axis=0,ignore_index=True)\n",
    "    \n",
    "    if not prediction_x_data.empty:\n",
    "        pred_t = []\n",
    "        x_test = prediction_x_data\n",
    "        best_interval = best_cushion(x_test,x_test[col[col_index]],np.arange(0.1,10,0.1))\n",
    "        signal_series = signal(x_test,x_test[col[col_index]],time_period,eps,interval_around_eps=best_interval)\n",
    "        trends = trend_divider(x_test,signal_series)\n",
    "        trend_signal = trend_divider(signal_series,signal_series,signal=True)\n",
    "        \n",
    "        for i in range(len(trends)):\n",
    "            idx = trends[i].index\n",
    "            df = trends[i].reset_index(drop=True)\n",
    "            if not trend_signal[i]: \n",
    "                pred_test = stagnant_model.predict(df)\n",
    "                pred_test = pd.Series(pred_test, index=idx)\n",
    "                pred_t.append(pred_test)\n",
    "            else:\n",
    "                pred_test = non_stagnant_model.predict(df[col])\n",
    "                pred_test = pd.Series(pred_test, index=idx)\n",
    "                pred_t.append(pred_test)\n",
    "                \n",
    "\n",
    "        pred_test = pd.concat(pred_t,axis=0,ignore_index=True)\n",
    "    \n",
    "    if not prediction_x_data.empty:\n",
    "        return pred_test  \n",
    "    else:\n",
    "        return pred_train\n",
    "\n",
    "def best_cushion(df,col,range_cushion):\n",
    "    \"\"\"\n",
    "    Tests for multiple values of intervals around eps.\n",
    "    \n",
    "    The objective is to find the interval value that minimizes the total nb of trends.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    min_nb = 10\n",
    "    for i in range_cushion:\n",
    "        signal_series = signal(df,col,time_period=500,eps=10,interval_around_eps=i)\n",
    "        ref = [len(i) for i in trend_divider(df,signal_series)]\n",
    "        nb = sum([ref.count(i) for i in range(1,7)])\n",
    "        ref_idx = 0\n",
    "        if nb < min_nb:\n",
    "            min_nb = nb\n",
    "            ref_idx = i\n",
    "            ref_list = ref\n",
    "    return range_cushion[ref_idx]\n",
    "\n",
    "\n",
    "#Lasso regression\n",
    "def lasso_reg(data,y_data,features_data:pd.DataFrame,features_test_data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    simple lasso regression.\n",
    "    Returns the prediction on the trainset and the prediction on the testset.\n",
    "    \"\"\"\n",
    "    model_lasso = LassoCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "    results_lasso = model_lasso.fit(data,y_data.filter(features_data.index).to_numpy())\n",
    "\n",
    "    pred = pd.Series(model_lasso.predict(features_data.to_numpy()),index=features_data.index)\n",
    "    pred_test = pd.Series(model_lasso.predict(features_test_data.to_numpy()),index=features_test_data.index)\n",
    "    \n",
    "    return pred , pred_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data\\X_train_NHkHMNU.csv\")\n",
    "y = pd.read_csv(r\"data\\y_train_ZAN5mwg.csv\")\n",
    "df_test = pd.read_csv(r\"data\\X_test_final.csv\")\n",
    "\n",
    "n_rows_train = len(df_train)\n",
    "n_rows_test = len(df_test)\n",
    "\n",
    "df_train = df_train.set_index(\"ID\")\n",
    "df_y_train = y.set_index(\"ID\")[\"TARGET\"]\n",
    "df_test = df_test.set_index(\"ID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The distribution of the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Plot the histogram #######################\n",
    "plt.figure(figsize=[9, 4])\n",
    "plt.hist(df_y_train)\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "print(df_y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Check for missing data ####################### \n",
    "\n",
    "missing = []\n",
    "for col in df_train.columns:\n",
    "    n_missing_train = df_train[col].isnull().sum()\n",
    "    n_missing_test = df_test[col].isnull().sum()\n",
    "    if n_missing_train != 0 or n_missing_test!=0:\n",
    "        missing.append(col)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Extract the features with missing data and plot ####################### \n",
    "\n",
    "filtered = features_to_correct(df_train)\n",
    "filtered_test = features_to_correct(df_test)\n",
    "\n",
    "#get list of the union of all indexes where we have some missing data\n",
    "idx = pd.DataFrame(np.arange(0,len(filtered)).reshape(-1,1),index=filtered).index\n",
    "idx_test = pd.DataFrame(np.arange(0,len(filtered_test)).reshape(-1,1),index=filtered_test).index\n",
    "\n",
    "#filter and plot\n",
    "df_train.filter(idx,axis=0).sort_values('DAY_ID').drop(['DAY_ID'],axis=1)[missing].reset_index(drop=True).cumsum().plot(legend=False, figsize=(15,9))\n",
    "df_test.filter(idx_test,axis=0).sort_values('DAY_ID').drop(['DAY_ID'],axis=1)[missing].reset_index(drop=True).cumsum().plot(legend=False, figsize=(15,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data anymore !\n"
     ]
    }
   ],
   "source": [
    "# Fill the missing values using interpolation method\n",
    "for col in df_train.drop('COUNTRY',axis=1).columns:\n",
    "    df_train[col] = df_train[col].interpolate()\n",
    "    df_test[col] =  df_test[col].fillna(df_test[col].mean())\n",
    "    \n",
    "# Interpolation doesn't work on the starting values\n",
    "# We do another check and fill the remaining missing values using the mean of the feature\n",
    "msg = True\n",
    "for col in df_train.drop('COUNTRY',axis=1).columns:\n",
    "    #first check\n",
    "    n_missing_train = df_train[col].isnull().sum()\n",
    "    n_missing_test = df_test[col].isnull().sum()\n",
    "    if n_missing_train != 0 or n_missing_test != 0:    \n",
    "        print(\"Column %s : missing in train set = %s / %s. In test set %s / %s\" % (col, n_missing_train, df_train.shape[0], n_missing_test, df_test.shape[0]))\n",
    "        print(\"___________________________________________________________________\")\n",
    "    #fill with the mean\n",
    "    df_train[col] =  df_train[col].fillna(df_train[col].mean())\n",
    "    df_test[col] =  df_test[col].fillna(df_test[col].mean())\n",
    "    #final check\n",
    "    n_missing_train = df_train[col].isnull().sum()\n",
    "    n_missing_test = df_test[col].isnull().sum()\n",
    "    \n",
    "    if n_missing_train != 0 or n_missing_test != 0:\n",
    "        msg = False\n",
    "        print(\"Column %s : missing in train set = %s / %s. In test set %s / %s\" % (col, n_missing_train, df_train.shape[0], n_missing_test, df_test.shape[0]))\n",
    "if msg:\n",
    "    print(\"No missing data anymore !\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having filled all the missing data, we find that our dataset contains some outliers, so we correct for observations **7 standard deviations** away from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.drop('COUNTRY',axis=1).columns:\n",
    "    upper_limit = df_train[col].mean() + 7*df_train[col].std()\n",
    "    lower_limit = df_train[col].mean() - 7*df_train[col].std()\n",
    "    for j in df_train.index:\n",
    "        if df_train[col].loc[j] > upper_limit or df_train[col].loc[j] < lower_limit :\n",
    "            df_train[col].loc[j] = df_train[col].mean()\n",
    "\n",
    "for col in df_test.drop('COUNTRY',axis=1).columns:\n",
    "    upper_limit = df_test[col].mean() + 7*df_test[col].std()\n",
    "    lower_limit = df_test[col].mean() - 7*df_test[col].std()\n",
    "    for j in df_test.index:\n",
    "        if df_test[col].loc[j] > upper_limit or df_test[col].loc[j] < lower_limit :\n",
    "            df_test[col].loc[j] = df_test[col].mean()\n",
    "            \n",
    "df_train.filter(idx,axis=0).sort_values('DAY_ID').drop(['DAY_ID'],axis=1)[missing].reset_index(drop=True).cumsum().plot(legend=False, figsize=(15,9))\n",
    "df_test.filter(idx_test,axis=0).sort_values('DAY_ID').drop(['DAY_ID'],axis=1)[missing].reset_index(drop=True).cumsum().plot(legend=False, figsize=(15,9)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Features Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first started with the main features, checked how much of the variance its explaining.\n",
    "\n",
    "We start by adding the FR_RENEWABLE, DE_RENEWABLE features we already worked with in class. we then use the same logic to add the rest of the energy sources taking into account their ratios: we end up with the FR_OTHER and DE_OTHER.\n",
    "\n",
    "We will later on explore other features that could be useful such as:\n",
    "- Computing a feature that contains a weighing of the consumption of each country\n",
    "- Since Wind and Rain are zone related features, we created a feature containing the sum of both countries for each feature\n",
    "- Exponential features\n",
    "\n",
    "*! - We get rid of the mean for all the features keeping only the variations.*\n",
    "\n",
    "We then analyze features individually vs the target in order to assess if they're more helpful to us in their linear, squared or exponential form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features related to energy sources\n",
    "df_train[\"FR_RENEWABLE\"]            = ((df_train.FR_SOLAR/df_train.FR_SOLAR.mean())*0.0216 + (df_train.FR_WINDPOW*0.0634/df_train.FR_WINDPOW.mean())).fillna(0)\n",
    "df_test[\"FR_RENEWABLE\"]             = ((df_test.FR_SOLAR/df_test.FR_SOLAR.mean())*0.0216 + (df_test.FR_WINDPOW*0.0634/df_test.FR_WINDPOW.mean())).fillna(0)\n",
    "\n",
    "df_train[\"DE_RENEWABLE\"]            = ((df_train.DE_SOLAR/df_train.DE_SOLAR.mean())*0.09 + (df_train.DE_WINDPOW/df_train.DE_WINDPOW.mean())*0.204).fillna(0)\n",
    "df_test[\"DE_RENEWABLE\"]             = ((df_test.DE_SOLAR/df_test.DE_SOLAR.mean())*0.09 + (df_test.DE_WINDPOW/df_test.DE_WINDPOW.mean())*0.204).fillna(0)\n",
    "\n",
    "df_train[\"FR_OTHER\"]                = ((df_train.FR_NUCLEAR/df_train.FR_NUCLEAR.mean()) *0.7058 + (df_train.FR_HYDRO/df_train.FR_HYDRO.mean())*0.1116 + (df_train.FR_GAS/df_train.FR_GAS.mean())*0.0718 ).fillna(0)\n",
    "df_test[\"FR_OTHER\"]                 = ((df_test.FR_NUCLEAR/df_test.FR_NUCLEAR.mean()) *0.7058 + (df_test.FR_HYDRO/df_test.FR_HYDRO.mean())*0.1116 + (df_test.FR_GAS/df_test.FR_GAS.mean())*0.0718 ).fillna(0)\n",
    "\n",
    "df_train[\"DE_OTHER\"]                = ((df_train.DE_NUCLEAR/df_train.DE_NUCLEAR.mean()) *0.117 + (df_train.DE_COAL/df_train.DE_COAL.mean())*0.234 + (df_train.DE_LIGNITE/df_train.DE_LIGNITE.mean())*0.234 ).fillna(0)\n",
    "df_test[\"DE_OTHER\"]                 = ((df_test.DE_NUCLEAR/df_test.DE_NUCLEAR.mean()) *0.117 + (df_test.DE_COAL/df_test.DE_COAL.mean())*0.234 + (df_test.DE_LIGNITE/df_test.DE_LIGNITE.mean())*0.234 ).fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A - First we want to have an idea on the correlation between the features themselves and the features vs the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = copy.deepcopy(df_train)\n",
    "corr_df['y'] = df_y_train\n",
    "\n",
    "corr = corr_df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(13, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1., center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B - Regressing different versions of the features with the target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation allows us to first get an idea on the relationship of each feature with the target on multiple levels: linear, quadratic and exponential.\n",
    "\n",
    "At first the idea was to only get the 1st order. However, after checking the results of these & manually checking the graphs of each features, we decided to add the 2nd order in order to catch some hidden variations.\n",
    "\n",
    "Then again, after another round of checkups, we find features which contained some very stagnant values / very limited variations. This is why we add a comparison of the exponential of the features vs the target (with a first order linear regression)\n",
    "\n",
    "Throughout these comparisons, we use the $R^{2}$ estimate from the score function to have an idea on the usefulness of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### <!> careful not to rerun it once the exp features are added ####################\n",
    "############################## Takes some time to compute (10mn) ! #############################\n",
    "\n",
    "model = LinearRegression()\n",
    "y = df_y_train\n",
    "fig, axs = plt.subplots(len(df_train.drop(['DAY_ID','COUNTRY'],axis=1).columns), 3, figsize=(15, 100))\n",
    "dic = {}\n",
    "\n",
    "for idx_col,col in enumerate(df_train.drop(['DAY_ID','COUNTRY'],axis=1).columns):\n",
    "   \n",
    "    X = df_train[col].values.reshape(-1,1)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot a linear regression\n",
    "    r_sq = model.score(X, y)\n",
    "    dic[f'{col}'] = r_sq\n",
    "    sns.regplot(x=X, y=y, x_ci = 95, order=1, line_kws={\"color\": \"red\"}, ax=axs[idx_col,0])\n",
    "    axs[idx_col,0].set_title(f\"Regression on {col} with $100 x R^{2}$:\" +  str(round(100*r_sq,2)))\n",
    "    axs[idx_col,0].set(xlabel=col, ylabel=\"TARGET\")\n",
    "   \n",
    "    # Plot a polynomial (order 2) regression\n",
    "    r_sq = model.score(X**2, y)\n",
    "    dic[f'{col}_2'] = r_sq\n",
    "    sns.regplot(x=X, y=y, x_ci = 95, order=2, label = \" $100 x R^{2}$ :\" +  str(round(100*r_sq,2)) + \"$\", line_kws={\"color\": \"red\"}, ax=axs[idx_col,1])\n",
    "    axs[idx_col,1].set_title(f\"Regression Order 2 with $100 x R^{2}$ :\" +  str(round(100*r_sq,2)))\n",
    "    axs[idx_col,1].set(xlabel=col, ylabel=\"TARGET\")\n",
    "    \n",
    "    X = np.exp(X)\n",
    "    # Plot an exponential regression\n",
    "    r_sq = model.score(X, y)\n",
    "    dic[f'{col}_exp'] = r_sq\n",
    "    sns.regplot(x=X, y=y, x_ci = 95, order=1, label = \" $100 x R^{2}$ :\" +  str(round(100*r_sq,2)) + \"$\", line_kws={\"color\": \"red\"}, ax=axs[idx_col,2])\n",
    "    axs[idx_col,2].set_title(f\"Regression exp(X) with $100 x R^{2}$ :\" +  str(round(100*r_sq,2)))\n",
    "    axs[idx_col,2].set(xlabel=col, ylabel=\"TARGET\")\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> We add weather features cumulating the features of each country\n",
    "\n",
    "-> We select the features that manifest a good $R^{2}$ in their exponential form vs the target and we add such their exponentials as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"relative_CONSUMPTION\"]    = (0.5164 * (df_train.DE_CONSUMPTION /df_train.DE_CONSUMPTION.mean()) + 0.4835 * (df_train.FR_CONSUMPTION/df_train.FR_CONSUMPTION.mean())).fillna(0.)\n",
    "df_test[\"relative_CONSUMPTION\"]     = (0.5164 * (df_test.DE_CONSUMPTION/df_test.DE_CONSUMPTION.mean()) + 0.4835 * (df_test.FR_CONSUMPTION/df_test.FR_CONSUMPTION.mean())).fillna(0.)\n",
    "df_train[\"CUM_WINDPOW\"]             = ((df_train.DE_WINDPOW/df_train.DE_WINDPOW.mean()) + (df_train.FR_WINDPOW/df_train.FR_WINDPOW.mean())).fillna(0.)\n",
    "df_test[\"CUM_WINDPOW\"]              = ((df_test.DE_WINDPOW/df_test.DE_WINDPOW.mean()) + (df_test.FR_WINDPOW/df_test.FR_WINDPOW.mean())).fillna(0.)\n",
    "df_train[\"CUM_WIND\"]                = ((df_train.DE_WIND/df_train.DE_WIND.mean()) + (df_train.FR_WIND/df_train.FR_WIND.mean())).fillna(0.)\n",
    "df_test[\"CUM_WIND\"]                 = ((df_test.DE_WIND/df_test.DE_WIND.mean()) + (df_test.FR_WIND/df_test.FR_WIND.mean())).fillna(0.)\n",
    "df_train[\"CUM_RAIN\"]                = ((df_train.DE_RAIN/df_train.DE_RAIN.mean()) + (df_train.FR_RAIN/df_train.FR_RAIN.mean())).fillna(0.)\n",
    "df_test[\"CUM_RAIN\"]                 = ((df_test.DE_RAIN/df_test.DE_RAIN.mean()) + (df_test.FR_RAIN/df_test.FR_RAIN.mean())).fillna(0.)\n",
    "df_train[\"CUM_TEMP\"]                = ((df_train.DE_TEMP/df_train.DE_TEMP.mean()) + (df_train.FR_TEMP/df_train.FR_TEMP.mean())).fillna(0.)\n",
    "df_test[\"CUM_TEMP\"]                 = ((df_test.DE_TEMP/df_test.DE_TEMP.mean()) + (df_test.FR_TEMP/df_test.FR_TEMP.mean())).fillna(0.)\n",
    "\n",
    "\n",
    "df_train[\"EXP_exchange\"]            = np.exp(df_train.DE_FR_EXCHANGE)\n",
    "df_train[\"EXP_DE_GAS\"]              = np.exp(df_train.DE_GAS)\n",
    "df_train[\"EXP_FR_GAS\"]              = np.exp(df_train.FR_GAS)\n",
    "df_train[\"EXP_DE_NUCLEAR\"]          = np.exp(df_train.DE_NUCLEAR)\n",
    "df_train[\"EXP_FR_NUCLEAR\"]          = np.exp(df_train.FR_NUCLEAR)\n",
    "df_train[\"EXP_DE_LIGNITE\"]          = np.exp(df_train.DE_LIGNITE)\n",
    "df_train[\"EXP_DE_RAIN\"]             = np.exp(df_train.DE_RAIN)\n",
    "df_train[\"EXP_FR_RAIN\"]             = np.exp(df_train.FR_RAIN)\n",
    "\n",
    "\n",
    "df_test[\"EXP_exchange\"]             = np.exp(df_test.DE_FR_EXCHANGE)\n",
    "df_test[\"EXP_DE_GAS\"]               = np.exp(df_test.DE_GAS)\n",
    "df_test[\"EXP_FR_GAS\"]               = np.exp(df_test.FR_GAS)\n",
    "df_test[\"EXP_DE_NUCLEAR\"]           = np.exp(df_test.DE_NUCLEAR)\n",
    "df_test[\"EXP_FR_NUCLEAR\"]           = np.exp(df_test.FR_NUCLEAR)\n",
    "df_test[\"EXP_DE_LIGNITE\"]           = np.exp(df_test.DE_LIGNITE)\n",
    "df_test[\"EXP_DE_RAIN\"]              = np.exp(df_test.DE_RAIN)\n",
    "df_test[\"EXP_FR_RAIN\"]              = np.exp(df_test.FR_RAIN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III- Features selection and the various attempts at price modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for us will be to use the previously found results($R^{2}$ results, correlation heatmap) in order to start a very early features selection.\n",
    "\n",
    "Then we will use the spearmann correlation in order to check if the selected features hold some explanative power"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up still with a lot of features, some of which are most likely correlated or even highly correlated. \n",
    "\n",
    "In order to deal with this colinearity issue, we will start by regressing each of the features vs all of our features and identify the highly correlated features.\n",
    "-> this approach can be very labor intensive and requires a lot of manipulations and tweeking in order to have some correct results.\n",
    "\n",
    "We will proceed by using the seaborn Pairplot function in order to have a complete comparison for each feature vs all the others. This will allow us to:\n",
    "- Identify colinear features\n",
    "- Have a visual on the distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Careful with this cell, might take too long (1h) #######################\n",
    "\n",
    "initial_features = df_train.drop('DAY_ID',axis=1).columns\n",
    "corr_df = copy.deepcopy(df_train[initial_features])\n",
    "corr_df['y'] = df_y_train\n",
    "sns.pairplot(corr_df[initial_features],hue='COUNTRY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by picking the features ourselves for a simple linear regression model.\n",
    "\n",
    "The main logic for picking the features is as follows:\n",
    "- We pick the FR_CONSUMPTION since it contains nearly the same info as the target\n",
    "- We pick one EXCHANGE feature since they're both symetric\n",
    "- the DE_NET_EXPORT and FR_NET_EXPORT since they contain important info on the exports of each country (and imports since symetric)\n",
    "- Energy related features with Renewable and other energy sources\n",
    "- other features that we thought might add some explanative power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[['FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'DE_NET_EXPORT', \n",
    "                    'FR_NET_EXPORT', 'DE_RAIN', 'FR_HYDRO','DE_RESIDUAL_LOAD',\n",
    "                    'FR_RENEWABLE', 'DE_RENEWABLE', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "x_test = df_test[['FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'DE_NET_EXPORT', \n",
    "                    'FR_NET_EXPORT', 'DE_RAIN', 'FR_HYDRO','DE_RESIDUAL_LOAD',\n",
    "                    'FR_RENEWABLE', 'DE_RENEWABLE', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "clf = LinearRegression()\n",
    "pred_val = cross_val_predict(clf, x_train, df_y_train)\n",
    "pred_val = pd.Series(pred_val, index=df_y_train.index)\n",
    "\n",
    "data_df = x_train.to_numpy()\n",
    "\n",
    "# Ridge regression\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(data_df,df_y_train.to_numpy())\n",
    "\n",
    "pred_train = pd.Series(model_ridge.predict(x_train.to_numpy()),index=x_train.index)\n",
    "pred_test = pd.Series(model_ridge.predict(x_test.to_numpy()),index=x_test.index) \n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b') \n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Countinuing with the same logic : \n",
    "\n",
    "We get rid of the highly colinear features (leaving only one). We then perform a simple lasso regression in order to get rid of the hidden correlations between the features and end up with only the \"uncorrelated\" features that are most helpful in simulating the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_train[initial_features].drop('COUNTRY',axis=1).to_numpy()\n",
    "\n",
    "# Lasso regression for factor sensitivities\n",
    "model_lasso = LassoCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_lasso = model_lasso.fit(data_df,df_y_train.to_numpy())\n",
    "\n",
    "# Key features selected by the Lasso model\n",
    "lasso_selection = pd.DataFrame([results_lasso.coef_],index=['lasso'],columns=df_train[initial_features].drop('COUNTRY',axis=1).columns)\n",
    "lasso_factors = lasso_selection.T[lasso_selection.T != 0].dropna().index.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particularity with a Lasso regression is that it allows us to get rid of some features entirely but it also makes the model not so rigid in terms of features: **one small change in the model/observations can cause the Lasso regression to change the chosen features.** So our use here of the lasso allows us to get rid of the non necessary features. \n",
    "\n",
    "Now that we have the selected features, we will use them in a Ridge regression in order to have a regression that **uses all of these features and that is robust in terms of used features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[lasso_factors]\n",
    "x_test = df_test[lasso_factors]\n",
    "\n",
    "clf = LinearRegression()\n",
    "pred_val = cross_val_predict(clf, x_train, df_y_train)\n",
    "pred_val = pd.Series(pred_val, index=df_y_train.index)\n",
    "\n",
    "data_df = x_train.to_numpy()\n",
    "\n",
    "# Ridge regression using all the factors selected by the Lasso\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(data_df,df_y_train.to_numpy())\n",
    "\n",
    "pred_train = pd.Series(model_ridge.predict(x_train.to_numpy()),index=x_train.index)\n",
    "pred_test = pd.Series(model_ridge.predict(x_test.to_numpy()),index=x_test.index)\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum(),'r')\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a quick try to the XGBoost library and fearing that it will take a lot of time to calibrate such model -> we decided to give up on this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventy_percent_data = int(0.7 * len(x_train))\n",
    "x_train_set = x_train[:seventy_percent_data]\n",
    "x_val = x_train[seventy_percent_data:]\n",
    "y_train_set = df_y_train[:seventy_percent_data]\n",
    "y_val = df_y_train[seventy_percent_data:]\n",
    "\n",
    "# XGBOOST\n",
    "# linear regression\n",
    "\n",
    "reg = xgb.XGBRegressor(n_estimators= 1000, early_stopping_rounds=50, learning_rate = 0.01)\n",
    "reg.fit(x_train_set, y_train_set, eval_set=[(x_train_set, y_train_set), (x_val, y_val)], verbose=50)\n",
    "pred_train = reg.predict(x_train)\n",
    "pred_train = pd.Series(pred_train, index=df_y_train.index)\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after checking the features, we noticed that some of these had similar patterns as the target and so, we had the idea to create a signal based on some of these features and try to fit the model around them.\n",
    "\n",
    "The main **hypothesis** is that a big portion of the variations in the target are caused by a single feature(or a very limited nb of features)\n",
    "\n",
    "The logic is as follows:\n",
    "- We choose a single feature (or multiple features: list of features)\n",
    "- We go along the path of the feature and check if it gets out of a fixed interval around its current observation:\n",
    "    - if the difference between the evolution of the feature and the previous mean evolution is above a certain treshold (eps + cushion) we return True\n",
    "    - if its between (eps + cushion) and (eps - cushion) then we consider it to be stagnant\n",
    "    - otherwise we return False\n",
    "    -> the states True and False represents the states of the feature where its explosive and where its not.\n",
    "- Now that we have our \"signal\", we use it to perform various linear regressions alternating between states where we regress on all our features and other states where we regress only on the single feature.\n",
    "\n",
    "We obtain the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[['FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'DE_NET_EXPORT', \n",
    "                    'FR_NET_EXPORT', 'DE_RAIN', 'FR_HYDRO','DE_RESIDUAL_LOAD',\n",
    "                    'FR_RENEWABLE', 'DE_RENEWABLE', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "x_test = df_test[['FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'DE_NET_EXPORT', \n",
    "                    'FR_NET_EXPORT', 'DE_RAIN', 'FR_HYDRO','DE_RESIDUAL_LOAD',\n",
    "                    'FR_RENEWABLE', 'DE_RENEWABLE', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "best_interval = best_cushion(x_train,x_train.FR_CONSUMPTION,np.arange(0.1,10,0.1))\n",
    "signal_series = signal(x_train,x_train.FR_CONSUMPTION,time_period=500,eps=10,interval_around_eps=best_interval)\n",
    "trends = trend_divider(x_train,signal_series)\n",
    "trend_signal = trend_divider(signal_series,signal_series,signal=True)\n",
    "\n",
    "pred_train = regression(trends,trend_signal,df_y_train,col=['FR_CONSUMPTION','FR_HYDRO'],col_index=0)\n",
    "pred_test = regression(trends,trend_signal,df_y_train,col=['FR_CONSUMPTION','FR_HYDRO'],col_index=0,prediction_x_data = x_test)\n",
    "pred_test.index=x_test.index\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this approach is as follows: signal ok on trainset, but is too much of an overfit and it shows on the test set results.\n",
    "\n",
    "We will probably need to find a way to get rid of these types of classifications (maybe logistic regression on the feature's variance ?)\n",
    "\n",
    "This approach also takes into account the data as if its a time series. bad model for the dataset we have. \n",
    "\n",
    "The signal in itself is very rigid and doesn't really take into account the different variations of the target: \n",
    "- we maybe will need to create a third model that takes into account the stagnant periods on their own.\n",
    "- the usage of the absolute value may be false/incorrect in this scope, since usually in financial markets prices behave differently in positive vs negative performance periods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA & Variance\n",
    "\n",
    "Keeping the same mindset of trying to capture the variance using some techniques or using a specific set of features, we decide to use the PCA on the covariance matrix of all the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.pca import PCA as PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_train = df_train[['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_FR_EXCHANGE',\n",
    "       'DE_NET_EXPORT', 'FR_NET_EXPORT', 'DE_RAIN', 'FR_RAIN',\n",
    "       'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD','DE_RENEWABLE','FR_RENEWABLE',\n",
    "       'CUM_WINDPOW', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "x_test = df_test[['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_FR_EXCHANGE',\n",
    "       'DE_NET_EXPORT', 'FR_NET_EXPORT', 'DE_RAIN', 'FR_RAIN',\n",
    "       'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD','DE_RENEWABLE','FR_RENEWABLE',\n",
    "       'CUM_WINDPOW', 'FR_OTHER', 'DE_OTHER']]\n",
    "\n",
    "cov = x_train.cov()\n",
    "test_cov = x_test.cov()\n",
    "\n",
    "nb_fact = 3\n",
    "\n",
    "####################### Train Set #######################\n",
    "y = preprocessing.scale(cov.to_numpy())\n",
    "pca_out=PCA((y))\n",
    "pca_comp=pca_out.factors\n",
    "pca_eval=pca_out.eigenvals\n",
    "pca_var=pca_eval/np.sum(pca_eval)\n",
    "plt.bar(np.arange(len(pca_var)),pca_var)\n",
    "plt.show()\n",
    "\n",
    "print(sum(pca_var[:nb_fact]))\n",
    "x_variance_factors = pca_comp.T[:nb_fact]\n",
    "\n",
    "####################### Test Set #######################\n",
    "y = preprocessing.scale(test_cov.to_numpy())\n",
    "pca_out=PCA((y))\n",
    "pca_comp=pca_out.factors\n",
    "pca_eval=pca_out.eigenvals\n",
    "pca_var=pca_eval/np.sum(pca_eval)\n",
    "plt.bar(np.arange(len(pca_var)),pca_var)\n",
    "plt.show()\n",
    "\n",
    "print(sum(pca_var[:nb_fact]))\n",
    "test_variance_factors = pca_comp.T[:nb_fact]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = np.zeros((len(x_variance_factors),len(x_train)))\n",
    "test_factors = np.zeros((len(test_variance_factors),len(x_test)))\n",
    "for i in range(len(x_variance_factors)):\n",
    "    factors[i,:] = np.dot(x_variance_factors[i,:],x_train.to_numpy().T)\n",
    "    test_factors[i,:] = np.dot(test_variance_factors[i,:],x_test.to_numpy().T)\n",
    "\n",
    "# Ridge regression to model the covariance of the target using ALL the PCA features\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(factors.T,df_y_train.to_numpy()-df_y_train.mean())\n",
    "\n",
    "y_variance_pred = model_ridge.predict(factors.T)\n",
    "variance_pred = model_ridge.predict(test_factors.T)\n",
    "\n",
    "\n",
    "# Lasso regression to model the mean variations of the target using the most useful features\n",
    "model_ridge = LassoCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(x_train.to_numpy(),df_y_train.to_numpy()-df_y_train.std())\n",
    "\n",
    "y_trend_pred = model_ridge.predict(x_train.to_numpy())\n",
    "trend_pred = model_ridge.predict(x_test.to_numpy())\n",
    "\n",
    "\n",
    "# Set all the data in a dataframe for the next and final regression\n",
    "df_data = pd.Series(y_trend_pred, name='trend',index=df_y_train.index).to_frame()\n",
    "df_data['variance'] = y_variance_pred\n",
    "\n",
    "df_test_data = pd.Series(trend_pred, name='trend',index=x_test.index).to_frame()\n",
    "df_test_data['variance'] = variance_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression to make sure to use all the created features\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=7)\n",
    "results_ridge = model_ridge.fit(df_data.to_numpy(),df_y_train.to_numpy())\n",
    "\n",
    "pred_train = pd.Series(model_ridge.predict(df_data.to_numpy()),index=df_data.index)\n",
    "pred_test = pd.Series(model_ridge.predict(df_test_data.to_numpy()),index=df_test_data.index)\n",
    "\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has its merits, but I think that we are losing a lot of the information hidden in the data by going through so many transformations.\n",
    "\n",
    "It would be a better idea to do a different split, minimize the number of transformations and reintegrate the created features back into the final model which will be used for the predictions.\n",
    "\n",
    "We now decide to opt for a split per country, the features are selected using previous results. We apply two different Lasso regressions to the two datasets and then regroup everything using a ridge regression along with some other key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_train = copy.deepcopy(df_train[df_train.COUNTRY == 'DE'].drop('COUNTRY',axis=1))\n",
    "fr_train = copy.deepcopy(df_train[df_train.COUNTRY == 'FR'].drop('COUNTRY',axis=1))\n",
    "\n",
    "de_test = copy.deepcopy(df_test[df_test.COUNTRY == 'DE'].drop('COUNTRY',axis=1))\n",
    "fr_test = copy.deepcopy(df_test[df_test.COUNTRY == 'FR'].drop('COUNTRY',axis=1))\n",
    "\n",
    "\n",
    "fr_df_features = fr_train[['FR_HYDRO', 'EXP_FR_NUCLEAR', 'DE_FR_EXCHANGE',\n",
    "       'DE_NET_EXPORT', 'FR_NET_IMPORT', 'FR_COAL', 'COAL_RET', 'CUM_WINDPOW', 'CUM_WIND']]\n",
    "de_df_features = de_train[['EXP_DE_LIGNITE', 'EXP_DE_RAIN', 'FR_TEMP',\n",
    " 'FR_NET_EXPORT', 'DE_NET_EXPORT', 'FR_RAIN',\n",
    " 'FR_DE_EXCHANGE', 'EXP_DE_GAS', 'DE_COAL',\n",
    " 'FR_RESIDUAL_LOAD', 'CUM_WINDPOW', 'CUM_WIND']]\n",
    "\n",
    "fr_test_features = fr_test[['FR_HYDRO', 'EXP_FR_NUCLEAR', 'DE_FR_EXCHANGE',\n",
    "       'DE_NET_EXPORT', 'FR_NET_IMPORT', 'FR_COAL', 'COAL_RET', 'CUM_WINDPOW', 'CUM_WIND']]\n",
    "de_test_features = de_test[['EXP_DE_LIGNITE', 'EXP_DE_RAIN', 'FR_TEMP',\n",
    " 'FR_NET_EXPORT', 'DE_NET_EXPORT', 'FR_RAIN',\n",
    " 'FR_DE_EXCHANGE', 'EXP_DE_GAS', 'DE_COAL',\n",
    " 'FR_RESIDUAL_LOAD', 'CUM_WINDPOW', 'CUM_WIND']]\n",
    "\n",
    "fr_data = fr_df_features.to_numpy()\n",
    "de_data = de_df_features.to_numpy()\n",
    "\n",
    "fr_pred, fr_pred_test = lasso_reg(fr_data,df_y_train,fr_df_features,fr_test_features)\n",
    "de_pred, de_pred_test = lasso_reg(de_data,df_y_train,de_df_features,de_test_features)\n",
    "\n",
    "\n",
    "# Selected features that will be added to the final model\n",
    "features = ['FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_RENEWABLE',\n",
    "        'FR_HYDRO', 'FR_NUCLEAR', 'DE_SOLAR', 'DE_WINDPOW',\n",
    "        'FR_WINDPOW','DE_RAIN', 'DE_NUCLEAR', 'FR_COAL',\n",
    "        'CUM_RAIN','DE_COAL']\n",
    "\n",
    "\n",
    "last = df_train[features]\n",
    "last_test = df_test[features]\n",
    "\n",
    "last['pred'] = np.nan\n",
    "last_test['pred'] = np.nan\n",
    "\n",
    "#goes through all the rows and checks if its in the FR or the DE predictions and replaces with its value\n",
    "for i in last.index:\n",
    "    if i in list(fr_pred.index):\n",
    "        last['pred'].loc[i] = fr_pred.loc[i]\n",
    "    elif i in list(de_pred.index):\n",
    "        last['pred'].loc[i] = de_pred.loc[i]\n",
    "else:\n",
    "    print(\"Allocated all values to their appropriate indexes successfully !\")\n",
    "\n",
    "for i in last_test.index:\n",
    "    if i in list(fr_pred_test.index):\n",
    "        last_test['pred'].loc[i] = fr_pred_test.loc[i]\n",
    "    elif i in list(de_pred_test.index):\n",
    "        last_test['pred'].loc[i] = de_pred_test.loc[i]\n",
    "else:\n",
    "    print(\"Allocated all values to their appropriate indexes successfully !\")\n",
    "    \n",
    "\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(last.to_numpy(),df_y_train.to_numpy())\n",
    "\n",
    "pred_train = pd.Series(model_ridge.predict(last.to_numpy()),index=last.index)\n",
    "pred_test = pd.Series(model_ridge.predict(last_test.to_numpy()),index=last_test.index)\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Final Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were hoping that the previous model would score the best, but sadly, it didn't. \n",
    "\n",
    "And so, we decide to go for a simple ridge regression using a set of selected features from previous results and see where it will take us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features \n",
    "features = ['FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_RENEWABLE',\n",
    "            'FR_HYDRO', 'FR_NUCLEAR', 'DE_SOLAR', 'DE_WINDPOW',\n",
    "            'FR_WINDPOW','DE_RAIN', 'DE_NUCLEAR', 'FR_COAL',\n",
    "            'CUM_RAIN','DE_COAL']\n",
    "\n",
    "last = df_train[features]\n",
    "last_test = df_test[features]\n",
    "\n",
    "model_ridge = RidgeCV(alphas=np.array([i for i in np.arange(1/1000,1,1/1000)]), fit_intercept=True, cv=20)\n",
    "results_ridge = model_ridge.fit(last.to_numpy(),df_y_train.to_numpy())\n",
    "\n",
    "pred_train = pd.Series(model_ridge.predict(last.to_numpy()),index=last.index)\n",
    "pred_test = pd.Series(model_ridge.predict(last_test.to_numpy()),index=last_test.index)\n",
    "\n",
    "plt.plot(pred_train.reset_index(drop=True).cumsum(),'b')\n",
    "plt.plot(pred_val.reset_index(drop=True).cumsum())\n",
    "plt.plot(df_y_train.reset_index(drop=True).cumsum(),'g')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pred_test.reset_index(drop=True).cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=(pred_test).to_frame(name='test').reset_index(drop=True)\n",
    "df['train'] = (pred_train).to_frame().reset_index(drop=True)\n",
    "df['y'] = df_y_train.reset_index(drop=True)\n",
    "sns.pairplot(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Spearman Rank Correlation\n",
    "spearman_train = spearmanr(df_y_train, pred_train)\n",
    "print(\"Spearman rank score on train dataset : %.4f\" % (spearman_train.correlation))\n",
    "\n",
    "# MSE\n",
    "mse_train = mean_squared_error(pred_train, df_y_train)\n",
    "print(\"MSE score on train dataset : %.4f\" % mse_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice similarities between the distribution of the target and that of the test set. And given the results (MSE & Spearman), we can say that this model can effectively predict with at least 20% accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Interpretation & Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this challenge, we took various approaches, some returned better results, but the overall challenge was very instructive.\n",
    "\n",
    "We started with a simple linear regression, penalized regressions, signals, used PCA factors as inputs in penalized regressions. We also corrected the data, got rid of all the outliers and used interpolation to fill the missing values.\n",
    "\n",
    "The various tried approches were explained and sadly didn't perform very well. We had to resort to simple approches in the end that surprisingly returned better results.\n",
    "\n",
    "-> This raises the question of why ? \n",
    "The reasons can be related to the dataset being obscure, not being a timeseries, not having more info on the data. Maybe the entire approach we had in feature selection wasn't correct and that maybe we needed to rank the observations given their values and analyse them starting from there."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "98fd2348db8c300cd9cc029810f780f8ae8d5c9ef0b7c3c4ad63c8a7ecc5adc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
